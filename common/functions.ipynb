{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.core.groupby.generic import DataFrameGroupBy\n",
    "import yaml\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dython import nominal\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import datetime\n",
    "from datetime import date\n",
    "\n",
    "from typing import List, Dict, Tuple, Union, Any\n",
    "sb.set()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_parameters()\n",
    "- input: path of yaml file\n",
    "- returns: parameters for the \n",
    "- description: will load the following parameters (in dictionary format):\n",
    "    - include_features\n",
    "    - Clean up\n",
    "    - Types of EDA\n",
    "    - \n",
    "- remarks: parameter file should contain the following details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parameters(folder_path: str) -> Dict[str, Union[List, Dict[str, str]]]:\n",
    "\n",
    "    parameters : Dict[str, Union[List, Dict[str, str]]] = yaml.safe_load(open(f\"{folder_path}/parameters.yaml\"))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_dataset()\n",
    "- input: parameters\n",
    "- returns: dataframe with datatypes formatted \n",
    "- description: Function will do the following tasks:\n",
    "    - load the dataset (in this case xxxxxxx.csv)\n",
    "    - load the yaml to format the features to the appropiate dataypes (in this case data_types.yaml)\n",
    "- remarks: \n",
    "    - Features are not mentioned in the data_types.yaml will be remained as default datatypes\n",
    "    - **Have not tested datasets of different features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(parameters: Dict[str, Any]) -> pd.DataFrame:\n",
    "\n",
    "    folder_path: str = parameters['dataset_location']\n",
    "\n",
    "    # Support multi dataset and load into one dataframe\n",
    "    def combined_datasets(datasets: List[str]) -> pd.DataFrame:\n",
    "        load_datasets: List[pd.DataFrame] = list()\n",
    "\n",
    "        for dataset in datasets:\n",
    "            # print(f\"dataset to load: {dataset}\")\n",
    "            temp_dataset: pd.DataFrame = pd.read_csv(f\"{folder_path}/{dataset}.csv\")\n",
    "            load_datasets.append(temp_dataset)\n",
    "\n",
    "        # print(f\"load_dataset size: {len(load_datasets)}\")\n",
    "\n",
    "        return pd.concat(load_datasets, ignore_index=True)\n",
    "    \n",
    "    # assign the data types preset in the data_types.yaml\n",
    "    def reassign_features_dataypes(dataset: pd.DataFrame) -> None:\n",
    "        config: Dict[str, str] = yaml.safe_load(open(f\"{folder_path}/data_types.yaml\"))\n",
    "        config_datatypes: Dict[str, str] = config['data_types']\n",
    "        dataset_features: List[str] = dataset.columns.to_list()\n",
    "        \n",
    "        for dataset_feature in dataset_features:\n",
    "            if dataset_feature not in config_datatypes.keys():\n",
    "                # print(f\"'{dataset_feature}' not in {config_datatypes.keys()}\")\n",
    "                continue            \n",
    "            dataset[dataset_feature] = dataset[dataset_feature].astype(config_datatypes[dataset_feature])\n",
    "\n",
    "    dataset = combined_datasets(parameters['datasets_to_load'])\n",
    "    # dataset.info()\n",
    "    # print()\n",
    "    reassign_features_dataypes(dataset)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean_up_dataset()\n",
    "- input: \n",
    "    - Dataframe\n",
    "    - parameters \n",
    "- returns: Dataframe\n",
    "- description: Function does the following to the dataset\n",
    "    - clean up \n",
    "    - drop row that contains NULL/NAN values\n",
    "    - extract the interested features\n",
    "- remarks: parameter input is dictionary data that loads from parameters.yaml file which contains all the configuration required for the clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_dataset(\n",
    "        dataframe: pd.DataFrame, \n",
    "        parameters: Dict[str, Any]\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "    def morph_feature_type(dataset: pd.DataFrame) -> None:\n",
    "        morph_feature_configs: Dict[str, str] = parameters['morph_feature_type']\n",
    "\n",
    "        for morph_feature, morph_feature_type in morph_feature_configs.items():\n",
    "            # print(f\"{morph_feature} with type {morph_feature_type}\")\n",
    "            # dataset[morph_feature].str.extract('(\\d+)').astype(int)\n",
    "            dataset[morph_feature]=dataset[morph_feature].str.extract('(\\d+)').astype(morph_feature_type)\n",
    "\n",
    "    def add_numerical_features(dataset: pd.DataFrame) -> None:\n",
    "        custom_numerical_features: List[Dict[str, Any]] = parameters['custom_numerical_features']\n",
    "        # print(f\"custom_numerical_features: {custom_numerical_features}\")\n",
    "\n",
    "        for new_feature in custom_numerical_features:\n",
    "            # print(f\"new_feature: {new_feature}, name: {list(new_feature.keys())[0]}, items: {new_feature.values()}\")\n",
    "\n",
    "            new_feature_name: str = list(new_feature.keys())[0]\n",
    "            derived_features: List[str] = new_feature[new_feature_name]['derived_features']\n",
    "            action_type: str = new_feature[new_feature_name]['action_type']\n",
    "            data_type: str = new_feature[new_feature_name]['data_type']\n",
    "            # print(f\"new_feature: {new_feature}, name: {new_feature_name}, derived_features: {derived_features}, action_type: {action_type}, data_type: {data_type}\")\n",
    "\n",
    "            dataset[new_feature_name] = pd.Series(dtype=data_type)\n",
    "\n",
    "            for index, feature in enumerate(derived_features):\n",
    "                print(f\"Add new column action= {index}:{feature}\")\n",
    "                if index == 0:\n",
    "                    dataset[new_feature_name] = dataset[feature]\n",
    "                \n",
    "                elif action_type == 'division':\n",
    "                    dataset[new_feature_name] = dataset[new_feature_name]//dataset[feature]\n",
    "\n",
    "        \n",
    "    ## EXTRACT FEATURE RELATED\n",
    "    # create a new dataframe to extract the interested feature (set in the parameters)\n",
    "    include_features_config: List[str] = parameters['include_features']\n",
    "    extracted_dataset = pd.DataFrame( dataframe[include_features_config] )\n",
    "\n",
    "    morph_feature_type(extracted_dataset)\n",
    "    add_numerical_features(extracted_dataset)\n",
    "\n",
    "    # print(f\"Empty row \\n{extracted_dataset.isnull().sum()}\")\n",
    "\n",
    "    return extracted_dataset.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_outlier_samples()\n",
    "- inputs: Dataframe\n",
    "- returns: Series of outliers based on supplied dataframe\n",
    "- description: identify the outliers based on the supplied dataframe\n",
    "- remarks: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlier_samples(dataframe: pd.DataFrame) -> pd.core.series.Series:\n",
    "\n",
    "    q1 = dataframe.quantile(0.25)\n",
    "    q3 = dataframe.quantile(0.75)\n",
    "    interquartile_range = q3-q1\n",
    "\n",
    "    lower_whisker = q1-1.5*interquartile_range\n",
    "    upper_whisker = q3+1.5*interquartile_range\n",
    "    outliers: pd.core.series.Series = ((dataframe < lower_whisker) | (dataframe > upper_whisker))\n",
    "\n",
    "    return outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_feature_outliers()\n",
    "- inputs: Dataframe\n",
    "- returns: None\n",
    "- description: prints number of outliers for every numerical features/column\n",
    "- remarks: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_outliers(dataframe: pd.DataFrame) -> None:\n",
    "\n",
    "    numerical_dataframe = dataframe.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "    for column in numerical_dataframe.columns:\n",
    "\n",
    "        outliers = sum(get_outlier_samples(numerical_dataframe[column]))\n",
    "        print(f\"[{column}] total outliers: {outliers}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove_outliers()\n",
    "- inputs: Dataframe\n",
    "- returns: Dataframe with outliers removed for every numerical feature/column \n",
    "- description: Function remove **UNION** outlier of the dataset. In other words remove the entire row containing outliers\n",
    "- remarks: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    numerical_dataframe = dataframe.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "    union_outliers = (get_outlier_samples(numerical_dataframe)).any(axis=1)\n",
    "    dataframe_with_outliers_removed: pd.DataFrame = dataframe[~union_outliers].reset_index(drop=True)\n",
    "\n",
    "    # print(f\"Total 'UNION' outliers: {sum(union_outliers)}\")\n",
    "    # instructor_names = pd.DataFrame(dataframe_with_outliers_removed['instructor_name'].value_counts(ascending=False))\n",
    "    # instructor_names.to_csv('instructor_names.csv')\n",
    "    return dataframe_with_outliers_removed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_numerical_eda_visualization()\n",
    "- inputs: \n",
    "    - dataframe\n",
    "    - List of catergorical features/columns which are numerical types\n",
    "    - plot title\n",
    "- returns: None\n",
    "- description: generates box, histo and violin plot for every numerical features (column) of the dataset\n",
    "- remakrs: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numerical_eda_visualization(\n",
    "        dataframe: pd.DataFrame,\n",
    "        plot_title: str\n",
    "    ) -> None:\n",
    "    \n",
    "    numerical_dataframe = dataframe.select_dtypes(include=['int64', 'float64'])\n",
    "    total_features = len(numerical_dataframe.columns)\n",
    "    figure, axes = plt.subplots(\n",
    "        total_features, \n",
    "        3, \n",
    "        figsize=(24,4.8*total_features)\n",
    "    )\n",
    "    \n",
    "    # figure.suptitle(plot_title, fontsize=20)\n",
    "    axes[0, 1].set_title(plot_title, fontsize=25)\n",
    "    row = 0\n",
    "    for column in numerical_dataframe.columns:\n",
    "        sb.boxplot(data=numerical_dataframe[column], orient='h', ax=axes[row,0])\n",
    "        sb.histplot(data=numerical_dataframe[column], ax=axes[row,1])\n",
    "        sb.violinplot(data=numerical_dataframe[column], orient='h', ax=axes[row,2])\n",
    "        row = row + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_numerical_heatmap() - MAY NEED TO REMOVE\n",
    "- inputs: \n",
    "    - dataframe\n",
    "    - plot title\n",
    "- returns: None\n",
    "- description: generates heatmap for every numerical features (column) of the dataset\n",
    "- remarks: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numerical_heatmap(\n",
    "        dataframe: pd.DataFrame,\n",
    "        plot_title: str\n",
    "    ) -> None:\n",
    "    \n",
    "    numerical_dataframe = dataframe.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    plt.title(plot_title, fontsize=20)\n",
    "    sb.heatmap(numerical_dataframe.corr(), vmin = -1, vmax = 1, linewidths = 1,\n",
    "        annot = True, fmt = \".2f\", annot_kws = {\"size\": 18}, cmap = \"RdBu\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_numerical_categorical_heatmap()\n",
    "- inputs: dataframe\n",
    "- returns: None\n",
    "- description: generates heatmap for numerical and catergorical features (column) of the dataset\n",
    " heatmap produces by calculate the correlation/strength-of-association of features in data-set with both categorical and continuous features using: \n",
    "    - Pearson's R for continuous-continuous cases \n",
    "    - Correlation Ratio for categorical-continuous cases \n",
    "    - Cramer's V or Theil's U for categorical-categorical cases\n",
    "- remarks: More info on the library checkout [dython](http://shakedzy.xyz/dython/modules/nominal/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numerical_categorical_heatmap(\n",
    "    dataframe: pd.DataFrame\n",
    ") -> None:\n",
    "    numerical_categorical_dataframe = dataframe.select_dtypes(exclude=['datetime64[ns]'])\n",
    "    nominal.associations(dataset=numerical_categorical_dataframe, figsize=(15, 13), title=\"Correlation/Strength-of-association of features\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_categorical_count_visualization()\n",
    "- input: \n",
    "    - Dataframe\n",
    "    - parameters \n",
    "- returns: None\n",
    "- description: generates categorical plot for selected catergorical features (column) of the dataset\n",
    "- remarks: parameter input is dictionary data that loads from parameters.yaml file which contains all the configuration required for the catergorical count visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_categorical_count_visualization(\n",
    "        dataframe: pd.DataFrame,\n",
    "        visualization_parameters: List[str]\n",
    "    ) -> None:\n",
    "\n",
    "    visualization_dataframe = pd.DataFrame( dataframe[visualization_parameters] )\n",
    "\n",
    "    for column in visualization_dataframe.columns:\n",
    "        category_total_types = len(dataframe[column].value_counts())\n",
    "        g = sb.catplot(y=column, data=visualization_dataframe, kind=\"count\", height=category_total_types)\n",
    "\n",
    "        for ax in g.axes.ravel():\n",
    "\n",
    "            for c in ax.containers:\n",
    "                ax.bar_label(c, label_type='edge')\n",
    "\n",
    "        plt.title(f\"Total count for all types available in '{column}' feature\", fontsize=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_numerical_vs_categorical_eda_visualization()\n",
    "- inputs: \n",
    "    - dataframe\n",
    "    - parameters\n",
    "    - plot title\n",
    "- returns: None\n",
    "- description: generates categorical plot for every catergorical features (column) of the dataset\n",
    "- remarks: parameter input is dictionary data that loads from parameters.yaml file which contains all the configuration required to generate eda visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numerical_vs_categorical_eda_visualization(\n",
    "        dataframe: pd.DataFrame,\n",
    "        visualization_parameters: Dict[str, str],\n",
    "        plot_title: str\n",
    "    ) -> None:\n",
    "\n",
    "    total_features = len(visualization_parameters)\n",
    "\n",
    "    figure, axes = plt.subplots(\n",
    "        total_features, \n",
    "        1, \n",
    "        figsize=(20,10*total_features),\n",
    "        constrained_layout=True\n",
    "    )\n",
    "    figure.tight_layout(pad=10.0)\n",
    "\n",
    "    row = 0\n",
    "    for numerical_feature, categorical_feature in visualization_parameters.items():\n",
    "        axes[row].set_title(\n",
    "            f\"{categorical_feature} boxplot based on {numerical_feature}\", \n",
    "            fontdict={'fontsize': 25, 'fontweight': 'medium'}\n",
    "        )\n",
    "\n",
    "        sb_plot = sb.boxplot(\n",
    "            y=numerical_feature, \n",
    "            x=categorical_feature, \n",
    "            data=dataframe, \n",
    "            order=dataframe.groupby(categorical_feature)[numerical_feature].median().sort_values().index,\n",
    "            ax=axes[row]\n",
    "        )\n",
    "        sb_plot.set_xticklabels(sb_plot.get_xticklabels(), rotation=40, ha='right')\n",
    "\n",
    "        row = row + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_catergorical_group_dataframe() [NOTE: PRIVATE FUNCTION]\n",
    "- inputs: \n",
    "    - dataframe\n",
    "    - observe_feature: numerical only\n",
    "    - groupby_features: categorical only\n",
    "    - observation_types: supported statistical types are ['sum', 'mean', 'median', 'min', 'max']\n",
    "- returns: grouped dataframe\n",
    "- description: generates grouped dataframe which can easily use pandas groupby method later on\n",
    "- remarks: currently does not support multi groupby features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_group_dataframe(\n",
    "        dataframe: pd.DataFrame,\n",
    "        observe_feature: str,\n",
    "        groupby_features: List[str],\n",
    "        observation_types: Dict[str, str]\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "    grouped_dataframe: pd.DataFrame = dataframe.groupby(groupby_features, as_index=False)[observe_feature].aggregate(list(observation_types.keys()))\n",
    "\n",
    "    return grouped_dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse_time_series_parameters() [NOTE: PRIVATE FUNCTION]\n",
    "- inputs: \n",
    "    - parameters\n",
    "- returns: Tuple\n",
    "    - observe_feature\n",
    "    - time_feature\n",
    "    - groupby_features\n",
    "    - observation_types\n",
    "    - visualization_features\n",
    "    - grouping_features\n",
    "- description: parse the time series parameters\n",
    "- remarks: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time_series_parameters(\n",
    "        parameters: Dict[str, Any]\n",
    "    ) -> Tuple[str, str, List[str], Dict[str, str], List[str], List[str]]:\n",
    "\n",
    "    #parse parameter\n",
    "    observe_feature: str = parameters['observe_feature']\n",
    "    time_feature: str = parameters['time_feature']\n",
    "    groupby_features: List[str] = parameters['groupby_features']\n",
    "    observation_types: Dict[str, str] = parameters['observation_types']\n",
    "\n",
    "    # for function input\n",
    "    visualization_features: List[str] = [time_feature] + [observe_feature] + groupby_features\n",
    "    grouping_features: List[str] = [time_feature] + groupby_features\n",
    "\n",
    "    return observe_feature, time_feature, groupby_features, observation_types, visualization_features, grouping_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_categorical_group_time_series_visualization()\n",
    "- inputs: \n",
    "    - dataframe\n",
    "    - parameters\n",
    "- returns: None\n",
    "- description: generate time series graph based on **EVERY** group of a categorical feature\n",
    "- remarks: currently does not support multi groupby features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_categorical_group_time_series_visualization(dataframe: pd.DataFrame, parameters: List[Dict[str, Any]]) -> None:\n",
    "\n",
    "    def generate_group_time_series_graph(\n",
    "            dataframe: pd.DataFrame,\n",
    "            time_feature: str,\n",
    "            observe_feature: str,\n",
    "            groupby_features: List[str],\n",
    "            observation_types: Dict[str, str]\n",
    "        ) -> None:\n",
    "\n",
    "        group_list: List[str] = np.unique(dataframe[groupby_features].values).tolist()\n",
    "\n",
    "        group_size: int = len(group_list)\n",
    "        grouped_dataframe: DataFrameGroupBy = dataframe.groupby(groupby_features) \n",
    "\n",
    "        figure, axes = plt.subplots(group_size, 1, figsize = (20, 8*group_size), constrained_layout=True)\n",
    "        \n",
    "        for row, group_name in enumerate(group_list):\n",
    "            group_dataframe = grouped_dataframe.get_group(group_name)\n",
    "        \n",
    "            axes[row].set_title(\n",
    "                f\"{group_name} {observe_feature} trend\", \n",
    "                fontdict={'fontsize': 25, 'fontweight': 'medium'}\n",
    "            )\n",
    "            axes[row].set_xlabel(time_feature)\n",
    "            axes[row].set_ylabel(observe_feature)\n",
    "\n",
    "            for observation_type, observation_type_color in observation_types.items():\n",
    "                sb.lineplot( \n",
    "                    x = time_feature, \n",
    "                    y = observation_type, \n",
    "                    color = observation_type_color, \n",
    "                    data = group_dataframe, \n",
    "                    ax = axes[row],\n",
    "                    label =f'{observation_type} value'\n",
    "                )\n",
    "\n",
    "            axes[row].legend(loc='best')\n",
    "    \n",
    "\n",
    "    for parameter in parameters:\n",
    "        \n",
    "        (observe_feature, time_feature, groupby_features, observation_types, visualization_features, grouping_features) = parse_time_series_parameters(parameter)\n",
    "\n",
    "        visualization_dataframe: pd.DataFrame = dataframe[visualization_features].copy()\n",
    "\n",
    "        grouped_visualization_dataframe = generate_group_dataframe(visualization_dataframe, observe_feature, grouping_features, observation_types)\n",
    "        generate_group_time_series_graph(grouped_visualization_dataframe, time_feature, observe_feature, groupby_features, observation_types)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_categorical_time_series_visualization()\n",
    "- inputs: \n",
    "    - dataframe\n",
    "    - parameters\n",
    "- returns: None\n",
    "- description: generate time series graph based on **ALL** group of a categorical feature\n",
    "- remarks: currently does not support multi groupby features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_categorical_time_series_visualization(dataframe: pd.DataFrame, parameters: List[Dict[str, Any]]) -> None:\n",
    "    \n",
    "    def generate_categorical_time_series_graph(\n",
    "            dataframe: pd.DataFrame,\n",
    "            time_feature: str,\n",
    "            observe_feature: str,\n",
    "            groupby_features: List[str],\n",
    "            observation_type: str = 'mean'\n",
    "        ) -> None:\n",
    "\n",
    "        group_list: List[str] = np.unique(dataframe[groupby_features].values).tolist()\n",
    "        # print(f\"group_list type: {type(group_list)}, content: \\n{group_list}\")\n",
    "\n",
    "        grouped_dataframe: DataFrameGroupBy = dataframe.groupby(groupby_features) \n",
    "\n",
    "        figure, axes = plt.subplots(1, 1, figsize = (30, 20), constrained_layout=True)\n",
    "        \n",
    "        axes.set_title(\n",
    "            f\"Overall {observe_feature} trend\", \n",
    "            fontdict={'fontsize': 25, 'fontweight': 'medium'}\n",
    "        )\n",
    "        axes.set_xlabel(time_feature)\n",
    "        axes.set_ylabel(observe_feature)\n",
    "\n",
    "        for group_name in group_list:\n",
    "            group_dataframe: pd.DataFrame = grouped_dataframe.get_group(group_name)            \n",
    "\n",
    "            sb.lineplot( \n",
    "                x = time_feature, \n",
    "                y = observation_type,\n",
    "                data = group_dataframe, \n",
    "                ax = axes,\n",
    "                label =f'{group_name}'\n",
    "            )\n",
    "\n",
    "            last_data: pd.DataFrame = group_dataframe.tail(1)\n",
    "            axes.text(x=last_data[time_feature], y=last_data[observation_type], s=group_name, va=\"center\")\n",
    "\n",
    "        axes.legend(loc='best')\n",
    "\n",
    "\n",
    "    for parameter in parameters:\n",
    "        \n",
    "        (observe_feature, time_feature, groupby_features, observation_types, visualization_features, grouping_features) = parse_time_series_parameters(parameter)\n",
    "\n",
    "        visualization_dataframe: pd.DataFrame = dataframe[visualization_features].copy()\n",
    "\n",
    "        grouped_visualization_dataframe = generate_group_dataframe(visualization_dataframe, observe_feature, grouping_features, observation_types)\n",
    "        generate_categorical_time_series_graph(grouped_visualization_dataframe, time_feature, observe_feature, groupby_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_time_series_linear_regression()\n",
    "NOTE: \n",
    "- Need to find out if possible to generate linear_regression with catergorical variables\n",
    "- The reason for this is because there is correlation between\n",
    "    - resale_price vs [flat_type, flat_model]\n",
    "    - remaining_lease vs [town, flat_model]\n",
    "    - floor_area_sqm vs flat_type\n",
    "- Need to accept list of parameters to generate different types of linear_regression lines for different response and predictors\n",
    "\n",
    "links:\n",
    "    - https://stackoverflow.com/questions/34007308/linear-regression-analysis-with-string-categorical-features-variables\n",
    "    - https://investigate.ai/classification/scikit-learn-and-categorical-features/\n",
    "\n",
    "- inputs: \n",
    "    - dataframe\n",
    "    - parameters\n",
    "- returns: None\n",
    "- description: \n",
    "- remarks: currently does not support multi groupby features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series_linear_regression(dataframe: pd.DataFrame, parameters: List[Dict[str, str]]) -> None:\n",
    "    \n",
    "    def generate_linear_regression_model(\n",
    "            group_dataframe: pd.DataFrame,\n",
    "            group_name: str,\n",
    "            predictor_feature: str,\n",
    "            observation_type: str = 'mean'\n",
    "        ) -> None:\n",
    "\n",
    "        response_dataframe = pd.DataFrame(group_dataframe[observation_type])\n",
    "        predictor_dataframe = pd.DataFrame(group_dataframe[predictor_feature])\n",
    "\n",
    "        if response_dataframe.isnull().values.any():\n",
    "            response_dataframe.fillna(0, inplace=True)\n",
    "    \n",
    "        # # # linear regression will have problems with categorical data!\n",
    "        linear_regression = LinearRegression()\n",
    "        linear_regression.fit(predictor_dataframe, response_dataframe)\n",
    "\n",
    "        # # Coefficients of the Linear Regression line\n",
    "        print(f\"Linear regression model for '{group_name}:\")\n",
    "        print('Intercept of Regression \\t: b = ', linear_regression.intercept_)\n",
    "        print('Coefficients of Regression \\t: a = ', linear_regression.coef_)\n",
    "        print()\n",
    "\n",
    "    def generate_linear_regression_graph(\n",
    "        dataframe: pd.DataFrame,\n",
    "        response_feature: str,\n",
    "        predictor_feature: str,\n",
    "        groupby_features: List[str],\n",
    "        observation_type: str = 'mean'\n",
    "    ) -> None:\n",
    "\n",
    "        group_list: List[str] = np.unique(dataframe[groupby_features].values).tolist()\n",
    "        grouped_dataframe: DataFrameGroupBy = dataframe.groupby(groupby_features) \n",
    "        print(f\"grouped_dataframed columns: {dataframe.columns}\")\n",
    "\n",
    "        # figure, axes = plt.subplots(1, 1, figsize = (30, 30), constrained_layout=True)\n",
    "        figure, axes = plt.subplots(1, 1, figsize = (30, 30))\n",
    "        axes.set_title(\n",
    "            f\"Overall {response_feature} trend\", \n",
    "            fontdict={'fontsize': 25, 'fontweight': 'medium'}\n",
    "        )\n",
    "        # axes.set_xlabel(predictor_feature)\n",
    "        # axes.set_ylabel(response_feature)\n",
    "\n",
    "        for group_name in group_list:\n",
    "\n",
    "            \n",
    "\n",
    "            group_dataframe: pd.DataFrame = grouped_dataframe.get_group(group_name).copy()\n",
    "            # print(f\"[1] group_dataframe content:\\n{group_dataframe}\")\n",
    "\n",
    "            # response_dataframe = pd.DataFrame(group_dataframe[observation_type])\n",
    "            # predictor_dataframe = pd.DataFrame(group_dataframe[predictor_feature])\n",
    "\n",
    "            # if response_dataframe.isnull().values.any():\n",
    "            #     response_dataframe.fillna(0, inplace=True)\n",
    "\n",
    "            # # due to NOTE1 will dchange datetyime to ordinal\n",
    "            # predictor_dataframe_ordinal = pd.DataFrame()\n",
    "            # # predictor_dataframe_ordinal[predictor_feature]: pd.Series = pd.to_datetime(predictor_dataframe[predictor_feature]).apply(lambda date: date.toordinal)\n",
    "            # predictor_dataframe_ordinal[predictor_feature]: pd.Series = predictor_dataframe[predictor_feature].apply(lambda x: x.toordinal())\n",
    "            # print(f\"[ordinal] predictor_dataframe type: {type(predictor_dataframe_ordinal)},\\npredictor_dataframe:\\n{predictor_dataframe_ordinal}\")\n",
    "\n",
    "            # # FOR TESTING\n",
    "            # predictor_dataframe_ordinal_revert = pd.DataFrame()\n",
    "            # predictor_dataframe_ordinal_revert[predictor_feature] = predictor_dataframe_ordinal[predictor_feature].apply(datetime.datetime.toordinal)\n",
    "            # # predictor_dataframe_ordinal_revert[predictor_feature]: pd.Series = pd.to_datetime(predictor_dataframe_ordinal[predictor_feature], origin='ordinal')\n",
    "            # print(f\"[ordinal_revert] predictor_dataframe type: {type(predictor_dataframe_ordinal_revert)},\\npredictor_dataframe:\\n{predictor_dataframe_ordinal_revert}\")\n",
    "\n",
    "            # # # linear regression will have problems with categorical data!\n",
    "            # linear_regression = LinearRegression()\n",
    "            # linear_regression.fit(predictor_dataframe_ordinal, response_dataframe)\n",
    "\n",
    "            # # Coefficients of the Linear Regression line\n",
    "            # print('Intercept of Regression \\t: b = ', linear_regression.intercept_)\n",
    "            # print('Coefficients of Regression \\t: a = ', linear_regression.coef_)\n",
    "            # print()\n",
    "\n",
    "            # # # NOITE1 TypeError: cannot perform __rmul__ with this index type: DatetimeArray\n",
    "            # predictor_regression_line: pd.DataFrame = predictor_dataframe\n",
    "            # prediction_regression_line: pd.DataFrame = linear_regression.intercept_ + linear_regression.coef_ * predictor_dataframe_ordinal\n",
    "            # # # prediction_regression_line.columns[0] = observation_type\n",
    "            # # print(f\"[TYPE] predictor_regression_line: {type(predictor_regression_line)}, prediction_regression_line: {prediction_regression_line}\")\n",
    "            # print(f\"prediction_regression_line content: {prediction_regression_line}\")\n",
    "            # prediction_regression_line.info()\n",
    "\n",
    "\n",
    "            # plot\n",
    "            # plt.plot(train_set,label='trainingSet')\n",
    "\n",
    "\n",
    "            ################# SEABORN APPROACH1 ########################\n",
    "            # group_dataframe.insert(group_dataframe.shape[1],'row_count', group_dataframe.index.value_counts().sort_index().cumsum())\n",
    "            # print(f\"[2] group_dataframe content:\\n{group_dataframe}\")\n",
    "\n",
    "            # using inbuilt seaborn regplot          \n",
    "            # lin_reg_fig = sb.regplot(\n",
    "            #     # x = predictor_feature,\n",
    "            #     x = 'row_count',\n",
    "            #     y = observation_type,\n",
    "            #     data = group_dataframe,\n",
    "            #     ax = axes,\n",
    "            #     label =f'{group_name} regression line',\n",
    "            #     scatter=False\n",
    "            # )\n",
    "            # labels = [item.get_text() for item in lin_reg_fig.get_xticklabels()]\n",
    "            # lin_reg_fig.set_xticklabels(labels)\n",
    "            ################# SEABORN APPROACH 1########################\n",
    "\n",
    "\n",
    "            ################# SEABORN APPROACH 2 ########################\n",
    "            # group_dataframe[f\"{predictor_feature}_ordinal\"] = pd.to_datetime(group_dataframe[predictor_feature]).apply(lambda date: date.toordinal()) # element type <class 'numpy.int64'>\n",
    "            # group_dataframe[f\"{predictor_feature}_ordinal\"] = group_dataframe[predictor_feature].apply(pd.Timestamp.toordinal) # method 2\n",
    "            group_dataframe[f\"{predictor_feature}_ordinal\"] = group_dataframe[predictor_feature].apply(lambda date: date.toordinal()) # element type <class 'numpy.int64'>\n",
    "            # print(f\"[ORDINAL UPDATE] predictor_dataframe type: {type(group_dataframe[f'{predictor_feature}_ordinal'][0])},\\npredictor_dataframe:\\n{group_dataframe[f'{predictor_feature}_ordinal'][0]}\")\n",
    "            # print(f\"[ORDINAL UPDATE] predictor_dataframe type: {type(group_dataframe)},\\npredictor_dataframe:\\n{group_dataframe}\")\n",
    "\n",
    "            generate_linear_regression_model(group_dataframe, group_name, f\"{predictor_feature}_ordinal\", observation_type)\n",
    "\n",
    "            sb.regplot(\n",
    "                data = group_dataframe,\n",
    "                x = f\"{predictor_feature}_ordinal\",\n",
    "                y = observation_type,\n",
    "                ax = axes,\n",
    "                scatter=False,\n",
    "                ci=None,\n",
    "                label =f'{group_name}',\n",
    "            )\n",
    "\n",
    "            # causes the graph to shrink\n",
    "            # last_data: pd.DataFrame = group_dataframe.tail(1)\n",
    "            # axes.text(x=last_data[f\"{predictor_feature}_ordinal\"], y=last_data[observation_type], s=group_name, va=\"center\")\n",
    "\n",
    "            axes.set_xlabel(predictor_feature)\n",
    "            new_labels = [date.fromordinal(int(item)) for item in axes.get_xticks()]\n",
    "            # new_labels = [pd.Timestamp.fromordinal(int(item)) for item in linear_regression_figure.get_xticks()] #method 2\n",
    "            # print(f\"labels: {new_labels}\")\n",
    "            axes.set_xticklabels(new_labels)\n",
    "            ################# SEABORN APPROACH 2 ########################\n",
    "\n",
    "\n",
    "            # sb.lineplot( \n",
    "            #     x = predictor_feature, \n",
    "            #     y = predictor_feature,\n",
    "            #     data = prediction_regression_line, \n",
    "            #     ax = axes,\n",
    "            #     label =f'{group_name}'\n",
    "            # )\n",
    "            \n",
    "        #     new_labels = [date.fromordinal(int(item)) for item in axes.get_xticks()]\n",
    "        #     axes.set_xticklabels(new_labels)\n",
    "\n",
    "            # this line should produce the price_per_sqm trend (wavy lines) -> works\n",
    "            # sb.lineplot( \n",
    "            #     # x = predictor_feature, \n",
    "            #     x = f\"{predictor_feature}_ordinal\",\n",
    "            #     y = observation_type,\n",
    "            #     data = group_dataframe, \n",
    "            #     ax = axes,\n",
    "            #     # label =f'{group_name}'\n",
    "            # )\n",
    "\n",
    "            # last_data: pd.DataFrame = group_dataframe.tail(1)\n",
    "            # axes.text(x=last_data[predictor_feature], y=last_data[observation_type], s=group_name, va=\"center\")\n",
    "\n",
    "        axes.legend(loc='best')\n",
    "\n",
    "\n",
    "    print(f\"input parameters: {parameters}\")\n",
    "\n",
    "    for parameter in parameters:\n",
    "\n",
    "        # parse parameter\n",
    "        response_feature: str = parameter['response']\n",
    "        predictor_feature: List[str] = parameter['predictors']\n",
    "        groupby_features: List[str] = parameter['groupby_features']\n",
    "        observation_type: str = parameter['observation_type']\n",
    "        linear_regression_model_features: List[str] = [predictor_feature, response_feature] + groupby_features\n",
    "\n",
    "        # print(f\"response: {response_feature}, predictors_feature: {predictor_feature}, linear_regression_model_features: {linear_regression_model_features}, groupby_features: {groupby_features}, observation_type: {observation_type}\")\n",
    "\n",
    "        linear_regression_model_dataframe: pd.DataFrame = dataframe[linear_regression_model_features].copy()\n",
    "        \n",
    "        grouping_features: List[str] = [predictor_feature] + groupby_features\n",
    "        grouped_linear_regression_model_dataframe: pd.DataFrame = linear_regression_model_dataframe.groupby(grouping_features, as_index=False)[response_feature].aggregate([observation_type])\n",
    "        # print(f\"type: {type(grouped_linear_regression_model_dataframe)} vs content:\\n{grouped_linear_regression_model_dataframe}\")\n",
    "        # print(grouped_linear_regression_model_dataframe) # okay!\n",
    "\n",
    "        generate_linear_regression_graph(grouped_linear_regression_model_dataframe, response_feature, predictor_feature, groupby_features, observation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc1015",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
