{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dython import nominal\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from typing import List, Dict, Tuple, Set, Union, Any\n",
    "sb.set()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_parameters()\n",
    "- input: path of yaml file\n",
    "- returns: parameters for the \n",
    "- description: will load the following parameters (in dictionary format):\n",
    "    - include_features\n",
    "    - Clean up\n",
    "    - Types of EDA\n",
    "    - \n",
    "- remarks: parameter file should contain the following details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_parameters(folder_path: str) -> Dict[str, Union[List, Dict[str, str]]]:\n",
    "\n",
    "    parameters : Dict[str, Union[List, Dict[str, str]]] = yaml.safe_load(open(f\"{folder_path}/parameters.yaml\"))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_dataset()\n",
    "- input: parameters\n",
    "- returns: dataframe with datatypes formatted \n",
    "- description: Function will do the following tasks:\n",
    "    - load the dataset (in this case xxxxxxx.csv)\n",
    "    - load the yaml to format the features to the appropiate dataypes (in this case data_types.yaml)\n",
    "- remarks: \n",
    "    - Features are not mentioned in the data_types.yaml will be remained as default datatypes\n",
    "    - **Have not tested datasets of different features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(parameters: Dict[str, Any]) -> pd.DataFrame:\n",
    "\n",
    "    folder_path: str = parameters['dataset_location']\n",
    "\n",
    "    # Support multi dataset and load into one dataframe\n",
    "    def combined_datasets(datasets: List[str]) -> pd.DataFrame:\n",
    "        load_datasets: List[pd.DataFrame] = list()\n",
    "\n",
    "        for dataset in datasets:\n",
    "            # print(f\"dataset to load: {dataset}\")\n",
    "            temp_dataset: pd.DataFrame = pd.read_csv(f\"{folder_path}/{dataset}.csv\")\n",
    "            load_datasets.append(temp_dataset)\n",
    "\n",
    "        # print(f\"load_dataset size: {len(load_datasets)}\")\n",
    "\n",
    "        return pd.concat(load_datasets, ignore_index=True)\n",
    "    \n",
    "    # assign the data types preset in the data_types.yaml\n",
    "    def reassign_features_dataypes(dataset: pd.DataFrame) -> None:\n",
    "        config: Dict[str, str] = yaml.safe_load(open(f\"{folder_path}/data_types.yaml\"))\n",
    "        config_datatypes: Dict[str, str] = config['data_types']\n",
    "        dataset_features: List[str] = dataset.columns.to_list()\n",
    "        \n",
    "        for dataset_feature in dataset_features:\n",
    "            if dataset_feature not in config_datatypes.keys():\n",
    "                # print(f\"'{dataset_feature}' not in {config_datatypes.keys()}\")\n",
    "                continue            \n",
    "            dataset[dataset_feature] = dataset[dataset_feature].astype(config_datatypes[dataset_feature])\n",
    "\n",
    "    dataset = combined_datasets(parameters['datasets_to_load'])\n",
    "    # dataset.info()\n",
    "    # print()\n",
    "    reassign_features_dataypes(dataset)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean_up_dataset()\n",
    "- input: \n",
    "    - Dataframe\n",
    "    - parameters \n",
    "- returns: Dataframe\n",
    "- description: Function does the following to the dataset\n",
    "    - clean up \n",
    "    - drop row that contains NULL/NAN values\n",
    "    - extract the interested features\n",
    "- remarks: parameter input is dictionary data that loads from parameters.yaml file which contains all the configuration required for the clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_dataset(\n",
    "        dataframe: pd.DataFrame, \n",
    "        parameters: Dict[str, Any]\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "    def morph_feature_type(dataset: pd.DataFrame) -> None:\n",
    "        morph_feature_configs: Dict[str, str] = parameters['morph_feature_type']\n",
    "\n",
    "        for morph_feature, morph_feature_type in morph_feature_configs.items():\n",
    "            # print(f\"{morph_feature} with type {morph_feature_type}\")\n",
    "            # dataset[morph_feature].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "            dataset[morph_feature]=dataset[morph_feature].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "\n",
    "    ## EXTRACT FEATURE RELATED\n",
    "    # create a new dataframe to extract the interested feature (set in the parameters)\n",
    "    include_features_config: List[str] = parameters['include_features']\n",
    "    extracted_dataset = pd.DataFrame( dataframe[include_features_config] )\n",
    "    # extracted_dataset.info()\n",
    "\n",
    "    morph_feature_type(extracted_dataset)\n",
    "\n",
    "    ## DUPLICATE RELATED\n",
    "    # method to print duplicates on specific column\n",
    "    # print(f\"Duplicated instructors: \\n{dataframe['instructor_name'].value_counts(ascending=False)}\")\n",
    "    # instructor_names = pd.DataFrame(dataframe['instructor_name'].value_counts(ascending=False))\n",
    "    # instructor_names.to_csv('instructor_names.csv')\n",
    "\n",
    "    # method to print duplicates exists on specific column\n",
    "    # for columns in dataframe.columns:\n",
    "    #     print(f\"Duplicated {columns}: {dataframe[columns].duplicated().any()}\")\n",
    "\n",
    "    # total_duplicated_ids = dataframe[dataframe.duplicated('id', keep=False)]\n",
    "    # print(f\"Course with duplicated ids: {len(total_duplicated_ids)}\")\n",
    "    # add condition if there is duplicates\n",
    "\n",
    "    \n",
    "\n",
    "    return extracted_dataset.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_outlier_samples()\n",
    "- inputs: Dataframe\n",
    "- returns: Series of outliers based on supplied dataframe\n",
    "- description: identify the outliers based on the supplied dataframe\n",
    "- remarks: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlier_samples(dataframe: pd.DataFrame) -> pd.core.series.Series:\n",
    "\n",
    "    q1 = dataframe.quantile(0.25)\n",
    "    q3 = dataframe.quantile(0.75)\n",
    "    interquartile_range = q3-q1\n",
    "\n",
    "    lower_whisker = q1-1.5*interquartile_range\n",
    "    upper_whisker = q3+1.5*interquartile_range\n",
    "    outliers: pd.core.series.Series = ((dataframe < lower_whisker) | (dataframe > upper_whisker))\n",
    "\n",
    "    return outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print_feature_outliers()\n",
    "- inputs: Dataframe\n",
    "- returns: None\n",
    "- description: prints number of outliers for every numerical features/column\n",
    "- remarks: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_outliers(dataframe: pd.DataFrame) -> None:\n",
    "\n",
    "    numerical_dataframe = dataframe.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "    for column in numerical_dataframe.columns:\n",
    "\n",
    "        outliers = sum(get_outlier_samples(numerical_dataframe[column]))\n",
    "        print(f\"[{column}] total outliers: {outliers}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove_outliers()\n",
    "- inputs: Dataframe\n",
    "- returns: Dataframe with outliers removed for every numerical feature/column \n",
    "- description: Function remove **UNION** outlier of the dataset. In other words remove the entire row containing outliers\n",
    "- remarks: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    numerical_dataframe = dataframe.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "    union_outliers = (get_outlier_samples(numerical_dataframe)).any(axis=1)\n",
    "    dataframe_with_outliers_removed: pd.DataFrame = dataframe[~union_outliers].reset_index(drop=True)\n",
    "\n",
    "    # print(f\"Total 'UNION' outliers: {sum(union_outliers)}\")\n",
    "    # instructor_names = pd.DataFrame(dataframe_with_outliers_removed['instructor_name'].value_counts(ascending=False))\n",
    "    # instructor_names.to_csv('instructor_names.csv')\n",
    "    return dataframe_with_outliers_removed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_numerical_eda_visualization()\n",
    "- inputs: \n",
    "    - dataframe\n",
    "    - List of catergorical features/columns which are numerical types\n",
    "    - plot title\n",
    "- returns: None\n",
    "- description: generates box, histo and violin plot for every numerical features (column) of the dataset\n",
    "- remakrs: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numerical_eda_visualization(\n",
    "        dataframe: pd.DataFrame,\n",
    "        plot_title: str\n",
    "    ) -> None:\n",
    "    \n",
    "    numerical_dataframe = dataframe.select_dtypes(include=['int64', 'float64'])\n",
    "    total_features = len(numerical_dataframe.columns)\n",
    "    figure, axes = plt.subplots(\n",
    "        total_features, \n",
    "        3, \n",
    "        figsize=(24,4.8*total_features)\n",
    "    )\n",
    "    \n",
    "    # figure.suptitle(plot_title, fontsize=20)\n",
    "    axes[0, 1].set_title(plot_title, fontsize=25)\n",
    "    row = 0\n",
    "    for column in numerical_dataframe.columns:\n",
    "        sb.boxplot(data=numerical_dataframe[column], orient='h', ax=axes[row,0])\n",
    "        sb.histplot(data=numerical_dataframe[column], ax=axes[row,1])\n",
    "        sb.violinplot(data=numerical_dataframe[column], orient='h', ax=axes[row,2])\n",
    "        row = row + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_numerical_heatmap()\n",
    "- inputs: \n",
    "    - dataframe\n",
    "    - plot title\n",
    "- returns: None\n",
    "- description: generates heatmap for every numerical features (column) of the dataset\n",
    "- remarks: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numerical_heatmap(\n",
    "        dataframe: pd.DataFrame,\n",
    "        plot_title: str\n",
    "    ) -> None:\n",
    "    \n",
    "    numerical_dataframe = dataframe.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    plt.title(plot_title, fontsize=20)\n",
    "    sb.heatmap(numerical_dataframe.corr(), vmin = -1, vmax = 1, linewidths = 1,\n",
    "        annot = True, fmt = \".2f\", annot_kws = {\"size\": 18}, cmap = \"RdBu\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_numerical_categorical_heatmap()\n",
    "- inputs: dataframe\n",
    "- returns: None\n",
    "- description: generates heatmap for numerical and catergorical features (column) of the dataset\n",
    " heatmap produces by calculate the correlation/strength-of-association of features in data-set with both categorical and continuous features using: \n",
    "    - Pearson's R for continuous-continuous cases \n",
    "    - Correlation Ratio for categorical-continuous cases \n",
    "    - Cramer's V or Theil's U for categorical-categorical cases\n",
    "- remarks: More info on the library checkout [dython](http://shakedzy.xyz/dython/modules/nominal/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_numerical_categorical_heatmap(\n",
    "    dataframe: pd.DataFrame\n",
    ") -> None:\n",
    "    nominal.associations(dataset=dataframe, figsize=(15, 15), title=\"Correlation/Strength-of-association of features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc1015",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
